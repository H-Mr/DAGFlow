为了验证 Phase 1 的内核治理能力（超时、重试、降级），我们需要构造一个能够精确控制“失败行为”和“时间消耗”的单元测试。

这个测试不仅要跑通，还要能证明：

1. **重试真的发生了**（不仅仅是碰巧成功）。
2. **超时真的生效了**（能在指定时间切断任务）。
3. **兜底真的执行了**（在异常或超时后接管了流程）。

下面是一个成体系的测试类 `GovernanceTest`。

### 核心测试思路

我们将利用 `AtomicInteger` 来计数执行次数，验证重试逻辑；利用 `Thread.sleep` 模拟耗时，验证超时逻辑。

#### 测试场景规划

1. **场景 A：重试后成功 (Retry Success)**
   - **设定**：节点前 2 次必挂，第 3 次成功。配置 `maxRetries = 3`。
   - **预期**：DAG 执行成功，且节点内部实际执行了 3 次。
2. **场景 B：重试耗尽触发降级 (Retry Exhausted -> Fallback)**
   - **设定**：节点一直挂。配置 `maxRetries = 2`，并配置 Fallback 返回 "Recovered"。
   - **预期**：DAG 执行成功（被兜底），结果为 "Recovered"，且节点实际执行了 3 次（1次初始+2次重试）。
3. **场景 C：超时触发降级 (Timeout -> Fallback)**
   - **设定**：节点耗时 500ms。配置 `timeout = 200ms`，并配置 Fallback。
   - **预期**：DAG 快速返回兜底值，总耗时远小于 500ms（约 200ms+）。

---

### 深度解析：为什么这个测试证明了架构的成功？

1. **分层治理的验证**：
   - 在 `testRetrySuccess` 中，异常是在 `ResilientNodeProcessor` 内部被吞掉并重试的，外部的 `DAGExecutor` 根本感知不到（Future 没有抛异常）。这证明了**装饰器层**工作正常。
   - 在 `testNodeTimeout` 中，`DAGExecutor` 利用 Future 的机制在 200ms 时强行切断了等待，而不需要节点逻辑配合（尽管节点响应中断是好习惯）。这证明了**执行器层**的异步治理工作正常。
2. **异常传递链路的验证**：
   - 在 `testRetryExhaustedWithFallback` 中，异常从 `ResilientNodeProcessor` 抛出（重试耗尽），穿透到 `DAGExecutor` 的 `Future`，最后被 `exceptionally` 捕获并降级。这验证了整个异常冒泡链路是通畅的。
3. **Fail-Safe 验证**：
   - 所有测试最终都没有抛出 Exception 导致程序崩溃，而是返回了预期的结果（SuccessData, MockData, TimeoutFallback）。这证明了框架具备了**“将异常转化为业务值”**的能力，这是生产级中间件稳定性的核心特征。